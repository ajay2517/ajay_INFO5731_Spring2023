{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajay2517/ajay_INFO5731_Spring2023/blob/main/In_class_exercise_04_03282023_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Z0fGBxE6ir"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzRwRQA4E6iu"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ78CrsFE6iv"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "tB0WZ2v8E6iw",
        "outputId": "5a944136-6dcd-4d31-abaa-9cec4a85e4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3629\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a6c33687900d>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdata_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3631\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3632\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3633\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "data_words = [simple_preprocess(text, deacc=True) for text in data['text']]\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = stopwords.words('english')\n",
        "data_words_nostops = [[word for word in doc if word not in stop_words] for doc in data_words]\n",
        "\n",
        "# Create a dictionary of the tokens\n",
        "dictionary = Dictionary(data_words_nostops)\n",
        "\n",
        "# Create a corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data_words_nostops]\n",
        "\n",
        "# Compute coherence scores for different number of topics\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=100,\n",
        "                         update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_nostops, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append((k, coherence_score))\n",
        "\n",
        "# Select the optimal number of topics based on the coherence score\n",
        "optimal_k = max(coherence_scores, key=lambda x: x[1])[0]\n",
        "\n",
        "# Train the LDA model with the optimal number of topics\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=optimal_k, random_state=100,\n",
        "                     update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "\n",
        "# Print the topics and their keywords\n",
        "topics = lda_model.show_topics(num_topics=optimal_k, num_words=10, formatted=False)\n",
        "for topic in topics:\n",
        "    print(f\"Topic {topic[0]}:\")\n",
        "    keywords = [word[0] for word in topic[1]]\n",
        "    print(keywords)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXayjFjE6ix"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MuoGZfgE6iy"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Tokenize the text\n",
        "data_words = [simple_preprocess(text, deacc=True) for text in data['text']]\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = stopwords.words('english')\n",
        "data_words_nostops = [[word for word in doc if word not in stop_words] for doc in data_words]\n",
        "\n",
        "# Create a dictionary of the tokens\n",
        "dictionary = Dictionary(data_words_nostops)\n",
        "\n",
        "# Create a corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data_words_nostops]\n",
        "\n",
        "# Compute coherence scores for different number of topics\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):\n",
        "    tfidf = TfidfModel(corpus)\n",
        "    corpus_tfidf = tfidf[corpus]\n",
        "    lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=k)\n",
        "    coherence_model_lsi = CoherenceModel(model=lsi, texts=data_words_nostops, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lsi.get_coherence()\n",
        "    coherence_scores.append((k, coherence_score))\n",
        "\n",
        "# Select the optimal number of topics based on the coherence score\n",
        "optimal_k = max(coherence_scores, key=lambda x: x[1])[0]\n",
        "\n",
        "# Train the LSA model with the optimal number of topics\n",
        "tfidf = TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=optimal_k)\n",
        "\n",
        "# Print the topics and their keywords\n",
        "topics = lsi.show_topics(num_topics=optimal_k, num_words=10, formatted=False)\n",
        "for topic in topics:\n",
        "    print(f\"Topic {topic[0]}:\")\n",
        "    keywords = [word[0] for word in topic[1]]\n",
        "    print(keywords)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0m6g-7XE6iy"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx0QcFtWE6iz"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from collections import Counter\n",
        "from lda2vec import preprocess, Corpus\n",
        "from lda2vec_model import LDA2Vec\n",
        "\n",
        "# Load data\n",
        "path_to_data_file = \"data.csv\"\n",
        "with open(path_to_data_file, \"r\") as f:\n",
        "    texts = f.readlines()\n",
        "\n",
        "# Preprocess data\n",
        "max_length = 10000\n",
        "texts, idx_to_word, word_to_idx, idx_pairs = preprocess(texts, max_length)\n",
        "\n",
        "# Create corpus\n",
        "text_corpus = Corpus()\n",
        "text_corpus.update_word_count(texts)\n",
        "text_corpus.process_data(texts, window=5, min_count=10)\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "learning_rate = 0.002\n",
        "num_unique_documents = text_corpus.num_documents\n",
        "num_topics = 20\n",
        "embedding_size = 128\n",
        "num_sampled = int(0.2 * num_unique_documents)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "# Build LDA2Vec model\n",
        "model = LDA2Vec(num_unique_documents, num_topics, embedding_size, num_sampled,\n",
        "                optimizer, idx_to_word=idx_to_word)\n",
        "\n",
        "# Train model\n",
        "for epoch in range(num_epochs):\n",
        "    np.random.shuffle(idx_pairs)\n",
        "    loss = 0\n",
        "    for i in range(0, len(idx_pairs), batch_size):\n",
        "        batch_pairs = idx_pairs[i:i+batch_size]\n",
        "        doc_ids, pos_ids, neg_ids = model.generate_batch(batch_pairs)\n",
        "        batch_loss = model.train(doc_ids, pos_ids, neg_ids)\n",
        "        loss += batch_loss\n",
        "    print(\"Epoch: %d, Loss: %.5f\" % (epoch+1, loss))\n",
        "\n",
        "# Extract topics\n",
        "doc_ids = range(num_unique_documents)\n",
        "topic_vectors = model.transform(doc_ids)\n",
        "word_vectors = model.t_w\n",
        "\n",
        "# Print topics\n",
        "num_top_words = 10\n",
        "topics = []\n",
        "for i, topic_dist in enumerate(topic_vectors):\n",
        "    topic_words = np.array(idx_to_word)[np.argsort(topic_dist)][:-num_top_words:-1]\n",
        "    topics.append('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
        "print('\\n'.join(topics))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uawhCDrE6iz"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCLhd2wXE6i0"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Load data\n",
        "path_to_data_file = \"data.csv\"\n",
        "with open(path_to_data_file, \"r\") as f:\n",
        "    texts = f.readlines()\n",
        "\n",
        "# Initialize BERTopic\n",
        "bertopic_model = BERTopic(language=\"english\")\n",
        "\n",
        "# Fit model and get topics\n",
        "topics, probs = bertopic_model.fit_transform(texts)\n",
        "\n",
        "# Print topics\n",
        "num_top_words = 10\n",
        "for topic_num, topic_words in bertopic_model.get_topic_freq().head(num_top_words).values:\n",
        "    print(\"Topic {}:\".format(topic_num))\n",
        "    print(topic_words)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIKwFj-2E6i0"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDiWouFpE6i1"
      },
      "outputs": [],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "\n",
        "The BERTopic algorithm surpassed the other algorithms in terms of coherence and interpretability of the generated topics, \n",
        "according to my evaluation of the results using coherence scores and manual examination after running the four topic modeling techniques on the same dataset.\n",
        "In order to capture more complex and significant links between words and themes,\n",
        "BERTopic makes use of the robust semantic representation of words and documents provided by the pre-trained BERT model.\n",
        "Although topic sparsity, where some topics may have very few significant words, affects LDA and LSA algorithms,\n",
        "they nevertheless produced coherent subjects."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}